{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff3a0bc",
   "metadata": {},
   "source": [
    "# Assignment - Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97427520",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an \n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba69f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "Simple Linear Regression-\n",
    "\n",
    "Linear regression is graphically depicted using a straight line with the slope defining how the change in one variable\n",
    "impacts a change in the other. The y-intercept of a linear regression relationship represents the value of one variable\n",
    "when the value of the other is 0.In linear regression, every dependent value has a single corresponding independent \n",
    "variable that drives its value. For example, in the linear regression formula of y = 3x + 7, there is only one possible\n",
    "outcome of 'y' if 'x' is defined as 2.\n",
    "\n",
    "Multiple Regression-\n",
    "\n",
    "Multiple regression assumes there is not a strong relationship between each independent variable. It also assumes there is\n",
    "a correlation between each independent variable and the single dependent variable. Each of these relationships is weighted\n",
    "to ensure more impactful independent variables drive the dependent value by adding a unique regression coefficient to each\n",
    "independent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba727b9b",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in \n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b849a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "Mainly there are 7 assumptions taken while using Linear Regression:\n",
    "\n",
    "* Linear Model\n",
    "* No Multicolinearlity in the data\n",
    "* Homoscedasticity of Residuals or Equal Variances\n",
    "* No Autocorrelation in residuals\n",
    "* Number of observations Greater than the number of predictors\n",
    "* Each observation is unique\n",
    "* Predictors are distributed Normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Model-\n",
    "\n",
    "According to this assumption, the relationship between the independent and dependent variables should be linear. The\n",
    "reason behind this relationship is that if the relationship will be non-linear which is certainly is the case in the\n",
    "real-world data then the predictions made by our linear regression model will not be accurate and will vary from the\n",
    "actual observations a lot.\n",
    "\n",
    "No Multicolinearlity in the data-\n",
    "If the predictor variables are correlated among themselves, then the data is said to have a multicollinearity problem.\n",
    "But why is this a problem? The answer to this question is that high collinearity means that the two variables vary very \n",
    "similarly and contain the same kind of information. This will leads to redundancy in the dataset. Due to redundancy,\n",
    "only the complexity of the model increase, and no new information or pattern is learned by the model. We generally try \n",
    "to avoid highly correlated features even while using complex models. We can identify highly correlated features using\n",
    "scatter plots or heatmap.\n",
    "\n",
    "Homoscedasticity of Residuals or Equal Variances-\n",
    "Homoscedasity is the term that states that the spread residuals which we are getting from the linear regression model\n",
    "should be homogeneous or equal spaces. If the spread of the residuals is heterogeneous then the model is called to be\n",
    "an unsatisfactory model. One can easily get an idea of the homoscedasticity of the residuals by plotting a scatter plot\n",
    "of the residual data.\n",
    "\n",
    "No Autocorrelation in residuals-\n",
    "\n",
    "One of the critical assumptions of multiple linear regression is that there should be no autocorrelation in the data.\n",
    "When the residuals are dependent on each other, there is autocorrelation. This factor is visible in the case of stock \n",
    "prices when the price of a stock is not independent of its previous one. Plotting the variables on a graph like a\n",
    "scatterplot or a line plot allows you to check for autocorrelations if any.\n",
    "\n",
    "Number of observations Greater than the number of predictors-\n",
    "\n",
    "For a better-performing model, the number of training data or observations should be always greater than the number of\n",
    "test or prediction data. However greater the number of observations better the model performance. Therefore, to build a\n",
    "linear regression model you must have more observations than the number of independent variables (predictors) in the data\n",
    "set. The reason behind this can be understood by the curse of dimensionality.\n",
    "\n",
    "Each observation is unique-\n",
    "\n",
    "It is also important to ensure that each observation is independent of the other observation.  Meaning each observation\n",
    "in the data set should be measured separately on a unique occurrence of the event that caused the observation.\n",
    "\n",
    "For example:  If you want to include two observations to measure the density of a liquid with 5 Kg mass and 5 L volume,\n",
    "    then you must experiment twice to measure the density for the two independent observations. Such observations are called\n",
    "    replicates of each other. It would be wrong to use the same measurement for both observations, as you will disregard\n",
    "    the random error.\n",
    "    \n",
    "Predictors are distributed Normally-\n",
    "\n",
    "This assumption ensures that you have equally distributed observations for the range of each predictor. So at the end of\n",
    "the model training, the predicted values for each test data should be a normal distribution. One can get an idea of the\n",
    "distribution of the predicted values by plotting density, KDE, or QQ plots for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd840430",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using \n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "In a linear regression model, the slope and intercept are key components that help describe the relationship between the\n",
    "independent variable (predictor) and the dependent variable (outcome). Here's how they can be interpreted:\n",
    "\n",
    "Intercept: This is the expected value of the dependent variable when the independent variable is zero. In other words,\n",
    "            it is the point where the regression line crosses the y-axis.\n",
    "\n",
    "Slope: This represents the change in the dependent variable for a one-unit change in the independent variable. It\n",
    "    indicates the direction and rate of change in the dependent variable as the independent variable increases or decreases.\n",
    "    \n",
    "#REAL WORLD EXAMPLE\n",
    "Let's consider a real-world scenario where we want to predict the price of houses based on their size (in square feet).\n",
    "\n",
    "Suppose we fit a linear regression model and obtain the following equation:\n",
    "Price = 50000 + 200 × Size\n",
    "\n",
    "Here:\n",
    "\n",
    "Intercept = 50000: This means that when the size of the house is 0 square feet (which isn't realistic but is the\n",
    "            theoretical baseline), the predicted price of the house is $50,000. This could be interpreted as the base\n",
    "            price of the house before considering its size.\n",
    "        \n",
    "Slope = 200: This indicates that for every additional square foot of house size, the price increases by $200."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f6aa2",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f884bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "Gradient Descent stands as a cornerstone orchestrating the intricate dance of model optimization. At its core, it is\n",
    "a numerical optimization algorithm that aims to find the optimal parameters—weights and biases—of a neural network by\n",
    "minimizing a defined cost function.\n",
    "Gradient Descent (GD) is a widely used optimization algorithm in machine learning and deep learning that minimises the\n",
    "cost function of a neural network model during training. It works by iteratively adjusting the weights or parameters of\n",
    "the model in the direction of the negative gradient of the cost function until the minimum of the cost function is reached.\n",
    "\n",
    "#HOW IT IS USED IN MACHINE LEARNING\n",
    "\n",
    "Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent\n",
    "in machine learning is simply used to find the values of a function's parameters (coefficients) that minimize a cost\n",
    "function as far as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18375eb",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between\n",
    "a single dependent continuous variable and more than one independent variable.\n",
    "\n",
    "Example:\n",
    "Prediction of CO2 emission based on engine size and number of cylinders in a car.\n",
    "\n",
    "#HOW IT IS DIFFERENT FROM SIMPLE LINEAR REGRESSION\n",
    "\n",
    "Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple\n",
    "explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship,\n",
    "multiple regression incorporates multiple independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbba85",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and \n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c89e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables (predictors)\n",
    "are highly correlated, meaning they have a linear relationship with each other. This can cause problems in estimating the\n",
    "coefficients of the regression model and in interpreting the results.\n",
    "\n",
    "#Issues Caused by Multicollinearity\n",
    "\n",
    "Unstable Coefficient Estimates: Multicollinearity can make the coefficient estimates very sensitive to small changes in\n",
    "    the model. This means that adding or removing a predictor can cause large changes in the estimated coefficients.\n",
    "\n",
    "Inflated Standard Errors: The standard errors of the coefficients become large, leading to wider confidence intervals and\n",
    "    less reliable significance tests. This can make it difficult to determine whether a predictor is actually significant.\n",
    "\n",
    "Difficulty in Determining the Effect of Each Predictor: When predictors are highly correlated, it is challenging to isolate\n",
    "    the individual effect of each predictor on the dependent variable.\n",
    "\n",
    "#Detecting Multicollinearity\n",
    "\n",
    "Correlation Matrix: Compute the correlation matrix for the independent variables. High correlation coefficients\n",
    "    (close to +1 or -1) between pairs of variables indicate multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF measures how much the variance of a regression coefficient is inflated due to\n",
    "    multicollinearity. A VIF value greater than 10 is often considered indicative of significant multicollinearity.\n",
    "\n",
    "Tolerance: Tolerance is the reciprocal of VIF. A tolerance value close to 0 indicates high multicollinearity.\n",
    "\n",
    "#Addressing Multicollinearity\n",
    "\n",
    "Remove Highly Correlated Predictors: If certain predictors are highly correlated, consider removing one of them from the \n",
    "                                     model.\n",
    "\n",
    "Combine Predictors: Create composite variables by combining correlated predictors through techniques like principal\n",
    "                    component analysis (PCA).\n",
    "\n",
    "Regularization Methods: Use regression techniques that include regularization, such as Ridge Regression or Lasso Regression.\n",
    "                    These methods add a penalty for large coefficients and can mitigate the effects of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64522173",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "\n",
    "Polynomial regression is an extension of linear regression that models the relationship between the independent variable x\n",
    "and the dependent variable y as an n-th degree polynomial. While linear regression models a straight-line relationship \n",
    "(first-degree polynomial) between x and y, polynomial regression can model more complex, curved relationships.  \n",
    "\n",
    "# Different from Linear Regression\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "Linear Regression: Models a linear relationship between the independent and dependent variables. \n",
    "Polynomial Regression: Models a non-linear relationship using a polynomial equation. This allows for capturing more\n",
    "                     complex patterns in the data.\n",
    "        \n",
    "Flexibility:\n",
    "\n",
    "Linear Regression: Limited to capturing straight-line relationships. It can only model linear trends.\n",
    "Polynomial Regression: More flexible in capturing the curvature in data. By increasing the degree of the polynomial (n), it\n",
    "    can model increasingly complex relationships.\n",
    "    \n",
    "Overfitting:\n",
    "\n",
    "Linear Regression: Less prone to overfitting because it fits a simple model.\n",
    "Polynomial Regression: Higher-degree polynomials can lead to overfitting, where the model fits the training data very\n",
    "    well but performs poorly on new, unseen data. This is because it can capture noise in the data as if it were a true\n",
    "    pattern.\n",
    "    \n",
    "Interpretability:\n",
    "\n",
    "Linear Regression: Coefficients are easy to interpret; each coefficient represents the change in the dependent variable\n",
    "    for a one-unit change in the independent variable.\n",
    "Polynomial Regression: Interpretation of coefficients becomes more complex as the degree increases, making it harder to \n",
    "    understand the influence of each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear \n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans8.\n",
    "\n",
    "#Advantages of Polynomial Regression\n",
    "\n",
    "Flexibility in Modeling Relationships: Polynomial regression can model a wider range of relationships between the independent\n",
    "    and dependent variables, including curves and more complex patterns that linear regression cannot capture.\n",
    "\n",
    "Better Fit for Non-Linear Data: When the relationship between variables is inherently non-linear, polynomial regression\n",
    "    can provide a better fit to the data, resulting in more accurate predictions.\n",
    "\n",
    "Captures Trends: Polynomial regression can effectively capture trends and cycles in the data, making it suitable for time\n",
    "    series analysis or data with seasonal patterns.\n",
    "    \n",
    "#Disadvantages of Polynomial Regression\n",
    "\n",
    "Risk of Overfitting: Higher-degree polynomials can overfit the training data, capturing noise and fluctuations as if they\n",
    "                  were true patterns. This results in poor generalization to new, unseen data.\n",
    "        \n",
    "Interpretability: As the degree of the polynomial increases, the model becomes more complex and the coefficients harder to\n",
    "               interpret. Understanding the contribution of each term becomes challenging.\n",
    "        \n",
    "Extrapolation Issues: Polynomial regression models can behave unpredictably outside the range of the training data\n",
    "     (extrapolation). High-degree polynomials, in particular, can produce extreme values.\n",
    "        \n",
    "Computational Complexity: Fitting higher-degree polynomials can be computationally more intensive and may require more\n",
    "    resources, especially with large datasets. \n",
    "\n",
    "#Situations Where Polynomial Regression is Preferred\n",
    "Non-Linear Relationships:\n",
    "\n",
    "When exploratory data analysis or domain knowledge suggests that the relationship between the independent and dependent\n",
    "variables is non-linear, polynomial regression can provide a more accurate model.\n",
    "\n",
    "Modeling Curves:\n",
    "\n",
    "If the data shows a clear curved pattern (e.g., quadratic or cubic trends), polynomial regression is suitable for capturing\n",
    "these patterns.\n",
    "\n",
    "Time Series Data with Trends and Cycles:\n",
    "\n",
    "Polynomial regression can be useful for modeling time series data that exhibits trends and cyclical patterns. For instance,\n",
    "in sales forecasting, demand prediction, or any other scenario where trends change over time.\n",
    "\n",
    "Complex Data Relationships:\n",
    "\n",
    "In fields like economics, biology, and engineering, where relationships between variables can be complex and non-linear,\n",
    "polynomial regression provides a useful tool for modeling these complexities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
