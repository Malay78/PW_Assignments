{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0852b75c",
   "metadata": {},
   "source": [
    "# Assignment- Feature Engineering 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188f3d7",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2366b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine\n",
    "learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their\n",
    "correlation with the outcome variable. The correlation is a subjective term here. For basic guidance, you can refer to\n",
    "the following table for defining correlation co-efficients.\n",
    "\n",
    "Steps in Filter Methods for feature selection-\n",
    "\n",
    "1. Evaluate the relevance of each feature using statistical measures such as Pearson’s Correlation, Linear Discriminant Analysis, Analysis of Variance, and Chi-Square methods.\n",
    "\n",
    "2.Rank the features based on their relevance scores.\n",
    "\n",
    "3.Select a threshold for relevance or a certain number of top-ranking features.\n",
    "\n",
    "4.Remove all features that fall below the threshold or are not in the top-ranking group.\n",
    "\n",
    "5.Use the remaining features to train the model.\n",
    "\n",
    "6.Repeat the process with different thresholds or numbers of top-ranking features to find the optimal set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748670d",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc87865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2\n",
    "The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance\n",
    "of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of\n",
    "feature by actually training a model on it.\n",
    "\n",
    "Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods\n",
    "pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics\n",
    "instead of cross-validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e958ef",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b773f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedded Methods-\n",
    "\n",
    "Embedded methods combined the advantages of both filter and wrapper methods by considering the interaction of features\n",
    "along with low computational cost. These are fast processing methods similar to the filter method but more accurate than\n",
    "the filter method.\n",
    "\n",
    "These methods are also iterative, which evaluates each iteration, and optimally finds the most important features that contribute the most to training in a particular iteration. Some techniques of embedded methods are:\n",
    "\n",
    "Regularization- \n",
    "\n",
    "Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the\n",
    "model. This penalty term is added to the coefficients; hence it shrinks some coefficients to zero. Those features with zero\n",
    "coefficients can be removed from the dataset. The types of regularization techniques are L1 Regularization (Lasso\n",
    "Regularization) or Elastic Nets (L1 and L2 regularization).\n",
    "\n",
    "Random Forest Importance -\n",
    "\n",
    "Different tree-based methods of feature selection help us with feature importance to provide\n",
    "a way of selecting features. Here, feature importance specifies which feature has more importance in model building or\n",
    "has a great impact on the target variable. Random Forest is such a tree-based method, which is a type of bagging algorithm\n",
    "that aggregates a different number of decision trees. It automatically ranks the nodes by their performance or decrease in\n",
    "the impurity (Gini impurity) over all the trees. Nodes are arranged as per the impurity values, and thus it allows to\n",
    "pruning of trees below a specific node. The remaining nodes create a subset of the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0895bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde40606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans4.\n",
    "The two main disadvantages of these methods are:\n",
    "1. The increasing overfitting risk when the number of observations is insufficient.\n",
    "2. The significant computation time when the number of variables is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a837b0",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature \n",
    "selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans5.\n",
    "Conditions for Wrapper Methods-\n",
    "\n",
    "1. It trains the algorithm by using the subset of featues iteratively.\n",
    "2. On the basis of the output of the model, features are added or subtracted, and with this feature set, the model has\n",
    "   trained again.\n",
    "3. Techniques- Forward Selection, Backward Elimination, Exhaustive Feature Selection, Recursive Feature Elimination\n",
    "\n",
    "Conditions for Filter Methods-\n",
    "\n",
    "1. Features are selected on the basis of statistics measures.\n",
    "2. This method does not depend on the learning algorithm and chooses the features as a pre- processing step.\n",
    "3. It filters out the irrelvant feature and redundant columns from the model by using different metrics through ranking.\n",
    "4. Techniques- Information Gain, Chi-Square Test, Fisher's Score, Missing Value Ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7749d",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. \n",
    "You are unsure of which features to include in the model because the dataset contains several different \n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4164cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans6.\n",
    "Information Gain: \n",
    "Information gain determines the reduction in entropy while transforming the dataset. It can be used as a feature selection\n",
    "technique by calculating the information gain of each variable with respect to the target variable.\n",
    "\n",
    "Chi-square Test: \n",
    "Chi-square test is a technique to determine the relationship between the categorical variables. The chi-square value is\n",
    "calculated between each feature and the target variable, and the desired number of features with the best chi-square value\n",
    "is selected.\n",
    "\n",
    "Fisher's Score:\n",
    "Fisher's score is one of the popular supervised technique of features selection. It returns the rank of the variable on the\n",
    "fisher's criteria in descending order. Then we can select the variables with a large fisher's score.\n",
    "\n",
    "Missing Value Ratio:\n",
    "The value of the missing value ratio can be used for evaluating the feature set against the threshold value. The formula\n",
    "for obtaining the missing value ratio is the number of missing values in each column divided by the total number of\n",
    "observations. The variable is having more than the threshold value can be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba914d2",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with \n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded \n",
    "method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans7.\n",
    "Regularization- Regularization adds a penalty term to different parameters of the machine learning model for avoiding\n",
    "overfitting in the model. This penalty term is added to the coefficients; hence it shrinks some coefficients to zero.\n",
    "Those features with zero coefficients can be removed from the dataset. The types of regularization techniques are L1\n",
    "Regularization (Lasso Regularization) or Elastic Nets (L1 and L2 regularization).\n",
    "\n",
    "Random Forest Importance - Different tree-based methods of feature selection help us with feature importance to provide\n",
    "a way of selecting features. Here, feature importance specifies which feature has more importance in model building or\n",
    "has a great impact on the target variable. Random Forest is such a tree-based method, which is a type of bagging algorithm\n",
    "that aggregates a different number of decision trees. It automatically ranks the nodes by their performance or decrease in\n",
    "the impurity (Gini impurity) over all the trees. Nodes are arranged as per the impurity values, and thus it allows to\n",
    "pruning of trees below a specific node. The remaining nodes create a subset of the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0ab64",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location, \n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important \n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the \n",
    "predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans8.\n",
    "In wrapper methodology, selection of features is done by considering it as a search problem, in which different\n",
    "combinations are made, evaluated, and compared with other combinations. It trains the algorithm by using the subset\n",
    "of features iteratively.\n",
    "\n",
    "On the basis of these requirements we will apply wrapper method-\n",
    "\n",
    "Forward selection - Forward selection is an iterative process, which begins with an empty set of features. After each\n",
    "iteration, it keeps adding on a feature and evaluates the performance to check whether it is improving the performance\n",
    "or not. The process continues until the addition of a new variable/feature does not improve the performance of the model.\n",
    "\n",
    "Backward elimination - Backward elimination is also an iterative approach, but it is the opposite of forward selection.\n",
    "This technique begins the process by considering all the features and removes the least significant feature. This elimination\n",
    "process continues until removing the features does not improve the performance of the model.\n",
    "\n",
    "Exhaustive Feature Selection- Exhaustive feature selection is one of the best feature selection methods, which evaluates\n",
    "each feature set as brute-force. It means this method tries & make each possible combination of features and return the\n",
    "best performing feature set.\n",
    "\n",
    "Recursive Feature Elimination-\n",
    "Recursive feature elimination is a recursive greedy optimization approach, where features are selected by recursively \n",
    "taking a smaller and smaller subset of features. Now, an estimator is trained with each set of features, and the importance\n",
    "of each feature is determined using coef_attribute or through a feature_importances_attribute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
