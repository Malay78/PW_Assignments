{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b42d6c6",
   "metadata": {},
   "source": [
    "# Regression-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972da594",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a3ab7",
   "metadata": {},
   "source": [
    "Ans1.\n",
    "Ridge regression is a procedure for eliminating the bias of coefficients and reducing the mean square error by shrinking\n",
    "the coefficients of a model towards zero in order to solve problems of overfitting or multicollinearity that are normally\n",
    "associated with ordinary least squares regression.\n",
    "\n",
    "It adds an additional term to the OLS loss function that pulls the estimating coefficients toward zero. This is done by\n",
    "adding a penalty term to the log likelihood, where this penalty term is governed by a parameter denoted as lambda (λ), \n",
    "thus lowering the variance of the model and increasing its stability as well as the robustness of the prediction made\n",
    "by the model.\n",
    "\n",
    "DIFFER FROM OTHER LEAST SQUARE REGRESSION-\n",
    "\n",
    "OLS regression aims for simple minimization of errors, potentially leading to overfitting, especially in the presence of multicollinearity.\n",
    "\n",
    "Ridge regression balances error minimization with coefficient shrinkage, making it more robust to multicollinearity and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009f607",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf57ef1",
   "metadata": {},
   "source": [
    "Ans2.\n",
    "Linearity:\n",
    "The relationship between the predictors (independent variables) and the response (dependent variable) is assumed to be linear.\n",
    "\n",
    "Independence:\n",
    "The observations are assumed to be independent of each other. This means the value of one observation does not influence\n",
    "or affect the value of another observation.\n",
    "\n",
    "Homoscedasticity:\n",
    "The residuals (errors) have constant variance at every level of the independent variable(s). In other words, the spread\n",
    "of the residuals should be consistent across all levels of the predictors.\n",
    "\n",
    "Multicollinearity:\n",
    "While Ridge Regression can handle multicollinearity better than Ordinary Least Squares (OLS) Regression, it assumes that\n",
    "multicollinearity is present to some extent. The presence of multicollinearity justifies the use of the L2 regularization\n",
    "term in Ridge Regression.\n",
    "\n",
    "Normality of Errors:\n",
    "The errors (residuals) are assumed to be normally distributed. This assumption is more critical for hypothesis testing\n",
    "and constructing confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8847861",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dae5ce",
   "metadata": {},
   "source": [
    "Ans3.\n",
    "Information Criteria (AIC/BIC):\n",
    "\n",
    "AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) can also be used to select lambda. These\n",
    "criteria balance model fit and complexity by penalizing the number of parameters. You compare models with different\n",
    "lambda values and choose the one with the lowest AIC or BIC value.\n",
    "\n",
    "Validation Set:\n",
    "If you have a separate validation set, you can train the model on the training set with different lambda values and\n",
    "evaluate the performance on the validation set. The lambda with the best validation performance is selected.\n",
    "\n",
    "Analytical Methods:\n",
    "In some cases, analytical methods or domain-specific knowledge can provide insights into appropriate lambda values,\n",
    "although this is less common.\n",
    "\n",
    "Leave-One-Out Cross-Validation (LOOCV):\n",
    "Similar to k-fold cross-validation but more computationally intensive. Each observation is used once as a validation\n",
    "while the remaining observations form the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37ba57",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3c0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients: [58.48258009 59.31372476 66.81462938 96.04857335 83.88581254 16.5289174\n",
      " 87.62033587  2.80575066 92.37691423 46.33472659]\n"
     ]
    }
   ],
   "source": [
    "#Ans4.\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1)\n",
    "\n",
    "# Fit a Ridge Regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = ridge.coef_\n",
    "print(\"Ridge Regression Coefficients:\", coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79db721",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7623e5c",
   "metadata": {},
   "source": [
    "Ans5.\n",
    "Ridge Regression performs particularly well in the presence of multicollinearity, which is one of its main advantages\n",
    "over Ordinary Least Squares (OLS) regression. Here's how it handles this situation:\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "Ridge Regression introduces a penalty term to the loss function, which shrinks the coefficients of correlated predictors.\n",
    "This shrinkage reduces the variance of the coefficient estimates, making the model more stable and less sensitive to the\n",
    "multicollinearity problem.\n",
    "\n",
    "Reduced Overfitting:\n",
    "By penalizing large coefficients, Ridge Regression helps prevent overfitting, which can occur when multicollinear\n",
    "predictors cause the model to overly rely on specific variables.\n",
    "\n",
    "Improved Prediction Accuracy:\n",
    "The regularization effect of Ridge Regression leads to more reliable and accurate predictions in the presence of\n",
    "multicollinearity, as it balances the trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268c2ef",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188b6d8",
   "metadata": {},
   "source": [
    "Ans6.\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables, but with some preprocessing steps\n",
    "for the categorical variables. Here’s how it works:\n",
    "\n",
    "Continuous Variables:\n",
    "Continuous (numerical) variables can be directly used in Ridge Regression.\n",
    "\n",
    "Categorical Variables:\n",
    "Categorical variables need to be converted into a numerical format before they can be used. This is usually done through\n",
    "one-hot encoding or label encoding.\n",
    "\n",
    "One-Hot Encoding: This creates binary (0/1) columns for each category in the variable, allowing the model to treat each\n",
    "    category as a separate feature.\n",
    "\n",
    "Label Encoding: This assigns a unique integer to each category. However, this can sometimes imply an ordinal relationship,\n",
    "    which may not be appropriate for all categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250bdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [-0.2003643]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Age': [25, 32, 47, 51, 62],\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "    'Income': [50000, 60000, 70000, 80000, 120000],\n",
    "    'Purchased': [0, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[['Age', 'Gender', 'Income']]\n",
    "y = df['Purchased']\n",
    "\n",
    "# Preprocessing pipeline for categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', ['Age', 'Income']),\n",
    "        ('cat', OneHotEncoder(), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Ridge Regression pipeline\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Ridge Regression model\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = ridge_pipeline.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfacc60b",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2ceeb",
   "metadata": {},
   "source": [
    "Ans7.\n",
    "Interpreting the coefficients in Ridge Regression is similar to interpreting coefficients in Ordinary Least Squares (OLS)\n",
    "regression, but with a few important nuances due to the regularization effect:\n",
    "\n",
    "Magnitude:\n",
    "The magnitude of the coefficients indicates the strength of the relationship between each independent variable and the\n",
    "dependent variable. Larger absolute values suggest a stronger influence on the predicted outcome.\n",
    "\n",
    "Direction:\n",
    "The sign of the coefficients (positive or negative) indicates the direction of the relationship. A positive coefficient\n",
    "means that as the independent variable increases, the dependent variable also increases, while a negative coefficient\n",
    "means that as the independent variable increases, the dependent variable decreases.\n",
    "\n",
    "Regularization Effect:\n",
    "Ridge Regression includes a penalty term that shrinks the coefficients towards zero but never exactly to zero. This\n",
    "shrinkage helps to mitigate multicollinearity and reduces the variance of the coefficient estimates. As a result, the\n",
    "coefficients in Ridge Regression are typically smaller in magnitude compared to those in OLS regression.\n",
    "\n",
    "Relative Importance:\n",
    "While Ridge Regression shrinks coefficients, the relative importance of the variables can still be assessed by comparing\n",
    "their magnitudes. Variables with larger coefficients (after regularization) are considered more influential.\n",
    "\n",
    "Bias-Variance Tradeoff:\n",
    "The introduction of the regularization parameter (lambda) adds bias to the coefficient estimates to reduce their variance.\n",
    "This tradeoff helps in creating a model that generalizes better to unseen data by preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae2f93",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a98a8",
   "metadata": {},
   "source": [
    "Ans8.\n",
    "Yes, Ridge Regression can indeed be used for time-series data analysis, but with some considerations and preprocessing \n",
    "steps to adapt the data to the Ridge Regression framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd3e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [14.23322422 14.84533552]\n",
      "Mean Squared Error: 16.337029660801292\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample time-series data\n",
    "data = {\n",
    "    'date': pd.date_range(start='2023-01-01', periods=10, freq='D'),\n",
    "    'value': [10, 12, 14, 13, 16, 15, 14, 17, 19, 18]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "ts_data = pd.DataFrame(data)\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "# Create lagged features\n",
    "ts_data['lag1'] = ts_data['value'].shift(1)\n",
    "ts_data['lag2'] = ts_data['value'].shift(2)\n",
    "ts_data = ts_data.dropna()  # Remove rows with NaN values\n",
    "\n",
    "# Features (lagged values)\n",
    "X = ts_data[['lag1', 'lag2']]\n",
    "# Target (current values)\n",
    "y = ts_data['value']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Create and fit the Ridge Regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = ridge.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# Optionally, you can also calculate and print the performance metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8908f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
