{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8af3c9e",
   "metadata": {},
   "source": [
    "Machine Learning 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070c32a",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how \n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ca364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans1.\n",
    "Overfitting-\n",
    "\n",
    "Overfitting occurs when a model learns to perform well on the training data but fails to generalize to unseen data \n",
    "(test/validation data). In other words, the model captures noise or random fluctuations in the training data as if\n",
    "they were meaningful patterns.\n",
    "Signs of overfitting include high accuracy on the training data but significantly lower accuracy on unseen data, and \n",
    "excessively complex models with many parameters.\n",
    "\n",
    "Underfitting-\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying structure of the data, resulting in poor\n",
    "performance on both the training and test/validation datasets.\n",
    "Signs of underfitting include low accuracy on both training\n",
    "and test/validation data, and a model that is too simplistic to capture the complexities of the data.\n",
    "\n",
    "#How they can be mitigated\n",
    "To mitigate overfitting, techniques such as cross-validation, regularization, early stopping, dropout, and reducing model\n",
    "complexity (e.g., feature selection, dimensionality reduction) can be employed.\n",
    "\n",
    "To address underfitting, one can try using more complex models, increasing the model's capacity \n",
    "(e.g., adding more layers or neurons), improving feature engineering, or collecting more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded18617",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa576b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans2.\n",
    "More Data: Increasing the size of your training dataset can help the model learn better generalizations from the data.\n",
    "\n",
    "Cross-Validation: Employ techniques like k-fold cross-validation to assess model performance. This helps in estimating\n",
    "                  how the model will perform on unseen data.\n",
    "\n",
    "Feature Selection: Select only the most relevant features that contribute to the prediction task. Removing irrelevant\n",
    "                    or redundant features can help reduce overfitting.\n",
    "Simplify the Model: Use simpler models with fewer parameters to reduce the risk of overfitting. For example, in decision\n",
    "                    trees, limit the maximum depth or prune the tree.\n",
    "\n",
    "Data Augmentation: Increase the diversity of your training data by applying transformations like rotation, scaling, or\n",
    "                cropping. This helps expose the model to a wider variety of patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f89263",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans3.\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture\n",
    "data complexities. It represents the inability of the model to learn the training data effectively result in poor\n",
    "performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially\n",
    "when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions.\n",
    "\n",
    "#Scenarios where underfitting can occur\n",
    "i) The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "ii) The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "iii) The size of the training dataset used is not enough.\n",
    "iv) Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "v) Features are not scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed235d",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and \n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a697967",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition\n",
    "and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high\n",
    "variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between\n",
    "both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is\n",
    "a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "Bias-\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct\n",
    "value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm \n",
    "should always be low-biased to avoid the problem of underfitting. \n",
    "\n",
    "Variance-\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of\n",
    "the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately\n",
    "on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error\n",
    "rates on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558d6ad",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. \n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting Detection\n",
    "The training set represents a majority of the available data (about 80%), and it trains the model. The test set represents\n",
    "a small portion of the data set (about 20%), and it is used to test the accuracy of the data it never interacted with\n",
    "before. By segmenting the dataset, we can examine the performance of the model on each set of data to spot overfitting\n",
    "when it occurs, as well as see how the training process works. The performance can be measured using the percentage of\n",
    "accuracy observed in both data sets to conclude on the presence of overfitting. If the model performs better on the training\n",
    "set than on the test set, it means that the model is likely overfitting.\n",
    "\n",
    "#Underfitting Detection\n",
    "Training and Validation Performance: If both training and validation performance are poor, it indicates underfitting. \n",
    "Training performance measures how well your model fits the training data, while validation performance measures its\n",
    "ability to generalize to unseen data. If both are low, your model likely lacks the capacity to capture the underlying\n",
    "patterns in the data.\n",
    "\n",
    "Learning Curves: Plot learning curves that show the model's performance (e.g., loss or accuracy) on the training and\n",
    "                validation sets as a function of training iterations or epochs. In underfitting scenarios, both training\n",
    "                and validation curves converge to a high error rate, indicating insufficient learning.\n",
    "\n",
    "Model Complexity: Compare the complexity of your model to the complexity of the problem. If your model is too simple\n",
    "                    relative to the complexity of the data, it may underfit. For example, if you're fitting a non-linear\n",
    "                    dataset with a linear model, it's likely to underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcfe68",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias \n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ecc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias-\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct\n",
    "value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm \n",
    "should always be low-biased to avoid the problem of underfitting. \n",
    "#Example for High Bias\n",
    "Linear Regression and Logistic Regression. \n",
    "\n",
    "Variance-\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of\n",
    "the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately\n",
    "on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error\n",
    "rates on test data.\n",
    "#Example for Low Bias\n",
    "Decision tree, Support Vector Machine, and K-nearest neighbours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fbbbd",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe \n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a692fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "While developing machine learning models you must have encountered a situation in which the training accuracy of the\n",
    "model is high but the validation accuracy or the testing accuracy is low. This is the case which is popularly known as\n",
    "overfitting in the domain of machine learning. Also, this is the last thing a machine learning practitioner would like\n",
    "to have in his model. In this article, we will learn about a method known as Regularization in Python which helps us to\n",
    "solve the problem of overfitting. But before that let’s understand what is the role of regularization in Python and what\n",
    "is underfitting and overfitting.\n",
    "\n",
    "#Regularizations Techniques\n",
    "i) L1 Regularization (Lasso):\n",
    "\n",
    "Adds the absolute value of the magnitude of coefficients as a penalty term to the loss function.\n",
    "Encourages sparsity in the model by driving some coefficients to zero.\n",
    "Helps in feature selection by automatically selecting the most relevant features.\n",
    "The regularization term is proportional to the sum of the absolute values of the model's coefficients.\n",
    "\n",
    "ii)L2 Regularization (Ridge):\n",
    "\n",
    "Adds the squared magnitude of coefficients as a penalty term to the loss function.\n",
    "Helps in preventing large fluctuations in the coefficient values.\n",
    "Encourages the distribution of the weight values across all features, reducing the impact of any single feature.\n",
    "The regularization term is proportional to the sum of the squares of the model's coefficients.\n",
    "iii) Elastic Net Regularization:\n",
    "\n",
    "Combines both L1 and L2 regularization terms.\n",
    "Allows for controlling the balance between L1 and L2 penalties using a hyperparameter.\n",
    "Addresses some of the limitations of L1 and L2 regularization individually.\n",
    "iv) Dropout: A technique commonly used in neural networks.\n",
    "            Randomly deactivates a fraction of neurons during each training iteration.\n",
    "            Helps prevent co-adaptation of neurons and encourages the network to learn more robust features.\n",
    "            Acts as a form of regularization by reducing model reliance on specific neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
