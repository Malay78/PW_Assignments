{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82c88f5",
   "metadata": {},
   "source": [
    "# Regression-3 (Lasso Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595ab70",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aefa6f",
   "metadata": {},
   "source": [
    "Ans1\n",
    "The Lasso Regression, a regression method based on Least Absolute Shrinkage and Selection Operator is quite an important\n",
    "technique in regression analysis for selecting the variables and regularization. It gets rid of irrelevant data features\n",
    "that help to prevent overfitting and features with weak influence become more cleanly identifiable because of shrinking\n",
    "the coefficients toward zero.\n",
    "\n",
    "How Lasso Regression is other from other regression techniques\n",
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, distinguishes itself from other\n",
    "regression techniques in a few key ways:\n",
    "\n",
    "Feature Selection: Unlike traditional linear regression, Lasso Regression performs both variable selection and\n",
    "    regularization. It can shrink the coefficients of less important features to exactly zero, effectively selecting\n",
    "    a simpler model with fewer predictors. This makes it particularly useful for models with high-dimensional data.\n",
    "\n",
    "Regularization: Lasso uses regularization, adding a penalty equal to the absolute value of the magnitude of coefficients\n",
    "    to the loss function. This encourages sparsity in the model by penalizing large coefficients, helping to prevent\n",
    "    overfitting.\n",
    "\n",
    "Handling Multicollinearity: By shrinking some coefficients to zero, Lasso can mitigate issues of multicollinearity,\n",
    "    which is common in datasets with highly correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d6d59",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab288a69",
   "metadata": {},
   "source": [
    "Ans2.\n",
    "Advantages of Lasso Regression\n",
    "Feature Selection: Lasso regression eliminates the need to manually select the most relevant features, hence, the developed\n",
    "    regression model becomes simpler and more explainable.\n",
    "    \n",
    "Regularization: Lasso constrains large coefficients, so a less biased model is generated, which is robust and general in\n",
    "    its predictions.\n",
    "    \n",
    "Interpretability: With lasso, models are often sparsity induced, therefore, they are easier to interpret and explain, which\n",
    "    is essential in fields like health care and finance.\n",
    "    \n",
    "Handles Large Feature Spaces: Lasso lends itself to dealing with high-dimensional data like we have in genomic as well as\n",
    "    imaging studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65431d17",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693334a",
   "metadata": {},
   "source": [
    "Ans3.\n",
    "\n",
    "Interpreting the coefficients of a Lasso Regression model involves understanding both the magnitude and the zero values of\n",
    "the coefficients:\n",
    "\n",
    "Magnitude: The size of a non-zero coefficient indicates the strength and direction of the relationship between the predictor\n",
    "    variable and the response variable. Larger magnitudes suggest stronger relationships.\n",
    "\n",
    "Zero Coefficients: Any coefficients that are exactly zero indicate that those predictors have been excluded from the model.\n",
    "    Lasso Regression performs automatic feature selection, which means it helps in identifying and eliminating less important\n",
    "    variables, resulting in a more parsimonious model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd21cd",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the \n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0857f78",
   "metadata": {},
   "source": [
    "Ans4.\n",
    "In Lasso Regression, the primary tuning parameter that can be adjusted is the regularization parameter, which controls\n",
    "the strength of the penalty applied to the coefficients. Here's how it affects the model's performance:\n",
    "\n",
    "Regularization Parameter:\n",
    "\n",
    "Low : A lower value of ùõº applies less penalty to the coefficients, allowing them to take larger values. This can lead to\n",
    "    a model that is more complex and may overfit the training data but can capture more intricate patterns.\n",
    "\n",
    "High : A higher value of increases the penalty on the coefficients, shrinking them more aggressively towards zero. This\n",
    "    can result in a simpler model that may underfit the data but is less likely to overfit, enhancing generalization to\n",
    "    unseen data.\n",
    "\n",
    "Optimal : The ideal value of balances bias and variance, leading to a model that generalizes well to new data. This value\n",
    "    is often determined using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e8806",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5e65a",
   "metadata": {},
   "source": [
    "Ans6.\n",
    "Ridge Regression:\n",
    "\n",
    "Type of Regularization: Uses ùêø2 regularization, which adds the sum of the squared coefficients as a penalty to the loss\n",
    "    function.\n",
    "\n",
    "Effect on Coefficients: It shrinks the coefficients towards zero but never exactly to zero. This means that all features\n",
    "    are retained in the model, even if their influence is minimized.\n",
    "\n",
    "Use Case: Ridge is suitable when you have many small, potentially important features and you want to avoid dropping any.\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "Type of Regularization: Uses ùêø1 regularization, which adds the sum of the absolute values of the coefficients as a penalty\n",
    "    to the loss function.\n",
    "\n",
    "Effect on Coefficients: It can shrink some coefficients exactly to zero, effectively performing feature selection by\n",
    "    excluding less important features from the model.\n",
    "\n",
    "Use Case: Lasso is useful when you suspect that only a few features are important, and you want to select them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d31c8",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc99ea",
   "metadata": {},
   "source": [
    "Ans7\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features. Here's how it works:\n",
    "\n",
    "Mechanism:\n",
    "\n",
    "Feature Selection: Lasso Regression performs feature selection by shrinking some of the coefficients exactly to zero. This\n",
    "    means that in the presence of multicollinearity (where two or more features are highly correlated), Lasso can effectively\n",
    "    reduce the influence of redundant features by setting their coefficients to zero, thereby removing them from the model.\n",
    "\n",
    "Impact:\n",
    "\n",
    "Reduced Complexity: By eliminating less important or redundant features, Lasso Regression reduces the complexity of the model.\n",
    "    This not only helps in dealing with multicollinearity but also improves the interpretability and performance of the model.\n",
    "\n",
    "Better Generalization: By selecting only the most important features and discarding the rest, Lasso Regression can improve the\n",
    "    model's ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac5053",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bf38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 0.08111308307896872\n",
      "Best cross-validated MSE: 1.8010149934258621\n"
     ]
    }
   ],
   "source": [
    "#Ans8.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [2, 3, 4, 5, 6],\n",
    "    'target': [1, 3, 2, 5, 4]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define X and y\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']\n",
    "\n",
    "# The rest of your code\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a range of lambda values\n",
    "lambda_values = np.logspace(-4, 4, 100)\n",
    "\n",
    "# Use cross-validation to find the optimal lambda\n",
    "best_score = float('inf')\n",
    "best_lambda = None\n",
    "for l in lambda_values:\n",
    "    lasso = Lasso(alpha=l)\n",
    "    scores = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    average_score = -np.mean(scores)\n",
    "    if average_score < best_score:\n",
    "        best_score = average_score\n",
    "        best_lambda = l\n",
    "\n",
    "# Train the final model with the optimal lambda\n",
    "final_model = Lasso(alpha=best_lambda)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "print(f\"Optimal lambda: {best_lambda}\")\n",
    "print(f\"Best cross-validated MSE: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a546f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
